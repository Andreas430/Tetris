{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependecies needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DQN_Agent import DQN_Agent\n",
    "from Tetris8x6 import Tetris\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import pickle\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DQN \n",
    "\n",
    "MAKE SURE YOU HAVE AN EMPTY FOLDER WITH THE NAME FOLDER IN YOUR CURRENT DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    running_scores = deque(maxlen=100)\n",
    "    agent = DQN_Agent()\n",
    "    env = Tetris(mode = 'glimpse')\n",
    "    total_actions = 0\n",
    "    finished = False\n",
    "    episode = 0\n",
    "\n",
    "    while not finished:\n",
    "        episode += 1\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            total_actions += 1\n",
    "            action = agent.choose_action(state)\n",
    "            new_state, reward, done = env.play(action)\n",
    "            agent.update_replay_buffer(action, state, new_state, done, reward)\n",
    "            agent.learn()\n",
    "            state = new_state\n",
    "            score += env.lines_cleared\n",
    "\n",
    "            if (total_actions % 5000) == 0:\n",
    "                agent.save_model(episode)\n",
    "                agent.save_buffer(episode)\n",
    "                t = time.localtime()\n",
    "                current_time = time.strftime('%H:%M:%S', t)\n",
    "                with open('checkpoint.txt', 'a') as checkpoint:\n",
    "                    checkpoint.write(\n",
    "                        f'Checkpoint :: Episode {episode},Frames={agent.frames},Epsilon={agent.epsilon},time : {current_time} \\n')\n",
    "\n",
    "        running_scores.append(score)\n",
    "        if (episode % 100) == 0:\n",
    "            t = time.localtime()\n",
    "            current_time = time.strftime('%H:%M:%S', t)\n",
    "            with open('log.txt', 'a') as logger:\n",
    "                logger.write(f'Episode: {episode} at time {current_time} with avg score {np.mean(running_scores)}\\n')\n",
    "\n",
    "        if np.mean(running_scores) >= 50:\n",
    "            finished = True\n",
    "            agent.save_model(episode)\n",
    "            agent.save_buffer(episode)\n",
    "            t = time.localtime()\n",
    "            current_time = time.strftime('%H:%M:%S', t)\n",
    "            with open('checkpoint.txt', 'a') as checkpoint:\n",
    "                checkpoint.write(\n",
    "                    f'FINISHED! Checkpoint :: Episode {episode},Frames={agent.frames},Epsilon={agent.epsilon},time : {current_time} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used in evaluating the mode\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "#creates a brand new NN model\n",
    "def create_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(8, 11,))\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv1D(32, 3, strides=3, activation=\"relu\",kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2))(inputs)\n",
    "    layer2 = layers.Conv1D(64, 2, strides=2, activation=\"relu\",kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2))(layer1)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer2)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\",kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2))(layer4)\n",
    "    action = layers.Dense(24, activation=\"linear\",kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2))(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "#Chooses the appropriate action based on the model\n",
    "def choose_action_test(model,state):\n",
    "    state_tensor = tf.convert_to_tensor(state)\n",
    "    state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "    # outputs the Q value for each action \n",
    "    Q_val = model(state_tensor, training=False)\n",
    "    # Take best action\n",
    "    action = tf.argmax(Q_val[0]).numpy()\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the agent\n",
    "model = create_model()\n",
    "model.load_weights('models/model_frames_1000000.h5')\n",
    "test_env = Tetris(mode = 'glimpse', render_mode = 'extra')\n",
    "state = test_env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "\n",
    "    action = choose_action_test(model,state)\n",
    "    new_state, reward, done = test_env.play(action, render = True, render_delay=0.5)\n",
    "#     test_env.render()\n",
    "    state = new_state\n",
    "    score += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record the perfomance of the agent\n",
    "results_dqn = []\n",
    "for i in range(5000,1005000,5000):\n",
    "    model = create_model()\n",
    "    model.load_weights('models/model_frames_'+str(i)+'.h5')\n",
    "    test_env = Tetris()\n",
    "    model_results = []\n",
    "    for i in range(10):\n",
    "        state = test_env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        total_lines = 0\n",
    "        singles = 0\n",
    "        doubles = 0\n",
    "        triples = 0\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            action = choose_action_test(model,state)\n",
    "            new_state, reward, done = test_env.play(action)\n",
    "            total_lines += test_env.lines_cleared\n",
    "            score += reward\n",
    "            if test_env.lines_cleared == 1:\n",
    "                singles += 1\n",
    "            elif test_env.lines_cleared ==2:\n",
    "                doubles+= 1\n",
    "            elif test_env.lines_cleared ==3:\n",
    "                triples +=1\n",
    "            state = new_state\n",
    "            \n",
    "        model_results.append([score, total_lines, singles, doubles, triples ])\n",
    "    results_dqn.append(np.mean(model_results,axis =0))\n",
    "\n",
    "pickle.dump(results_dqn, open('results_dqn.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the perfomance\n",
    "with open('results_dqn.p', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "x = [i for i in range(5000,1005000,5000)]\n",
    "plt.figure(figsize= (14,7))\n",
    "z = np.polyfit(x, [i[0] for i in results], 3)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x, [i[0] for i in results])\n",
    "plt.plot(x,p(x),\"r\")\n",
    "plt.plot(x,[0.16 for i in range(len(x))],'k--')\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "plt.xlabel('Num. Actions',size= 14)\n",
    "plt.ylabel('Average Reward',size= 14)\n",
    "plt.title('Glimpse reward',size= 14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize= (14,7))\n",
    "z = np.polyfit(x, [i[1] for i in results], 3)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x, [i[1] for i in results],label ='Lines Cleared')\n",
    "plt.plot(x,p(x),\"r\", label = 'Mean agent Perfomance')\n",
    "plt.plot(x,[0.16 for i in range(len(x))],'k--',label = 'Random agent')\n",
    "plt.legend()\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "plt.xlabel('Num. Actions',size= 14)\n",
    "plt.ylabel('Average cleared lines',size= 14)\n",
    "plt.title('DQN Glimpse',size= 14)\n",
    "plt.savefig('lines cleared', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize= (14,7))\n",
    "plt.plot(x, [i[2] for i in results], label = 'singles')\n",
    "plt.plot(x, [i[3] for i in results],'y', label = 'doubles')\n",
    "plt.plot(x, [i[4] for i in results],'r', label = 'triples')\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "plt.legend()\n",
    "plt.xlabel('Num. Actions',size= 14)\n",
    "plt.ylabel('Lines Cleared',size= 14)\n",
    "plt.savefig('sev', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
