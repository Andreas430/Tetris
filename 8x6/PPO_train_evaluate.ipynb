{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependecies needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tetris8x6 import Tetris\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gym\n",
    "import scipy.signal\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEY functions of the agent\n",
    "\n",
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def mlp(x, sizes, activation=tf.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Hyperparameters\n",
    "steps_per_epoch = 4000\n",
    "epochs = 250\n",
    "gamma = 0.60\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (64, 64)        \n",
    "render = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the environment and model architecture\n",
    "env = Tetris(mode = 'glimpse')\n",
    "state = env.reset()\n",
    "observation_dimensions = state.shape[0] * state.shape[1]#88\n",
    "num_actions = 6 * 4#24\n",
    "\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "observation_input = keras.Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "logits = mlp(observation_input, list(hidden_sizes) + [num_actions], tf.tanh, None)\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "value = tf.squeeze(\n",
    "    mlp(observation_input, list(hidden_sizes) + [1], tf.tanh, None), axis=1\n",
    ")\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "total_actions = 0\n",
    "for epoch in range(epochs):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        observation = observation.reshape(1, -1)\n",
    "        logits, action = sample_action(observation)\n",
    "        observation_new, reward, done= env.play(action[0].numpy())\n",
    "        total_actions += 1\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get the value and log-probability of the action\n",
    "        value_t = critic(observation)\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    # Print mean return and length for each epoch\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\")\n",
    "    actor.save('models/model_frames_'+ str(epoch)+ '.h5')\n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime('%H:%M:%S', t)\n",
    "    with open('checkpoint.txt', 'a') as checkpoint:\n",
    "        checkpoint.write(\n",
    "        f'Checkpoint :: Epoch {epoch},Actions={total_actions},reward : {sum_return / num_episodes},time : {current_time} \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used in evaluating the mode\n",
    "def create_model():\n",
    "    observation_input = keras.Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "    logits = mlp(observation_input, list(hidden_sizes) + [num_actions], tf.tanh, None)\n",
    "    model = keras.Model(inputs=observation_input, outputs=logits)\n",
    "    return model\n",
    "\n",
    "@tf.function\n",
    "def choose_action(observation,model):\n",
    "    logits = model(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the agent\n",
    "model = create_model()\n",
    "model.load_weights('models/model_frames_249.h5')\n",
    "test_env = Tetris(mode= 'glimpse', render_mode= 'extra')\n",
    "state = test_env.reset()\n",
    "state = state.reshape(1, -1)\n",
    "done = False\n",
    "total_lines = 0\n",
    "\n",
    "while not done:\n",
    "\n",
    "    _, action = choose_action(state,model)\n",
    "    new_state, reward, done = test_env.play(action[0].numpy(), render= True, render_delay= 0.02)\n",
    "    total_lines += test_env.lines_cleared\n",
    "    state = new_state.reshape(1,-1)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record the perfomance of the agent\n",
    "results_ppo = []\n",
    "for i in range(epochs):\n",
    "    model = create_model()\n",
    "    model.load_weights('models/model_frames_'+str(i)+'.h5')\n",
    "    test_env = Tetris()\n",
    "    model_results = []\n",
    "    for i in range(10):\n",
    "        state = test_env.reset()\n",
    "        state = state.reshape(1, -1)\n",
    "        done = False\n",
    "        score = 0\n",
    "        total_lines = 0\n",
    "        singles = 0\n",
    "        doubles = 0\n",
    "        triples = 0\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            _, action = choose_action(state,model)\n",
    "            new_state, reward, done = test_env.play(action[0].numpy())\n",
    "            total_lines += test_env.lines_cleared\n",
    "            score += reward\n",
    "            if test_env.lines_cleared == 1:\n",
    "                singles += 1\n",
    "            elif test_env.lines_cleared ==2:\n",
    "                doubles+= 1\n",
    "            elif test_env.lines_cleared ==3:\n",
    "                triples +=1\n",
    "            state = new_state.reshape(1,-1)\n",
    "            \n",
    "        model_results.append([score, total_lines, singles, doubles, triples ])\n",
    "    results_ppo.append(np.mean(model_results,axis =0))\n",
    "\n",
    "\n",
    "pickle.dump(results_ppo, open('results_ppo.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_ppo.p', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "results = results[:250]\n",
    "x = [i*4000 for i in range(epochs)]\n",
    "plt.figure(figsize= (14,7))\n",
    "z = np.polyfit(x, [i[0] for i in results], 5)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x, [i[0] for i in results])\n",
    "plt.plot(x,p(x),\"r\")\n",
    "plt.plot(x,[0.16 for i in range(len(x))],'k--')\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "plt.xlabel('Num. Actions',size= 14)\n",
    "plt.ylabel('Average Reward',size= 14)\n",
    "plt.title('Glimpse PPO',size= 14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize= (14,7))\n",
    "z = np.polyfit(x, [i[1] for i in results], 5)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x, [i[1] for i in results],label ='Lines Cleared')\n",
    "plt.plot(x,p(x),\"r\", label = 'Mean agent Perfomance')\n",
    "plt.plot(x,[0.16 for i in range(len(x))],'k--',label = 'Random agent')\n",
    "plt.legend()\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "plt.xlabel('Num. Actions',size= 14)\n",
    "plt.ylabel('Average cleared lines',size= 14)\n",
    "plt.title('Glimpse PPO',size= 14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize= (14,7))\n",
    "plt.plot(x,[i[2] for i in results], label = 'singles')\n",
    "plt.plot(x, [i[3] for i in results],'y', label = 'doubles')\n",
    "plt.plot(x,[i[4] for i in results],'r', label = 'triples')\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "plt.legend()\n",
    "plt.xlabel('Num. Actions',size= 14)\n",
    "plt.ylabel('Lines Cleared',size= 14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
